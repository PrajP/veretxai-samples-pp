{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c76faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.31.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (57.4.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.38.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bdb53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.7/site-packages (1.8.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components==0.1.5 in /opt/conda/lib/python3.7/site-packages (0.1.5)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.1.5) (1.4.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.1.5) (1.31.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.42.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in ./.local/lib/python3.7/site-packages (from kfp) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in ./.local/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.9 in ./.local/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in ./.local/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in ./.local/lib/python3.7/site-packages (from kfp) (0.10)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.34.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in ./.local/lib/python3.7/site-packages (from kfp) (1.12.8)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in ./.local/lib/python3.7/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in ./.local/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.16.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in ./.local/lib/python3.7/site-packages (from kfp) (1.2.12)\n",
      "Requirement already satisfied: kubernetes<13,>=8.0.0 in ./.local/lib/python3.7/site-packages (from kfp) (12.0.1)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.2)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (21.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (57.4.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (2021.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.19.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (3.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (1.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (2.24.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (1.38.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (1.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (1.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.3.0->google-cloud-pipeline-components==0.1.5) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (2.4.7)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (4.6.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.17.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.5.30)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.6)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pydantic<2,>=1.8.2->kfp) (3.10.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components==0.1.5) (4.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp) (3.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp google-cloud-pipeline-components==0.1.5 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f910427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"my-project-timeseries-pp\"\n",
    "\n",
    "DATA_BUCKET = 'new-hub-data-bucket'\n",
    "REGION = \"us-central1\"\n",
    "SETTING_FILE = 'Model_Settings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2c6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210907074707-bucket\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "BUCKET_NAME = TIMESTAMP + \"-bucket\"\n",
    "print(BUCKET_NAME)\n",
    "# !gsutil mb -l $REGION $BUCKET_NAME\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "bucket.create(location = REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e12ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "import google.cloud\n",
    "from google.cloud import aiplatform as ap\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb81707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://20210907074707-bucket/pipeline_root/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "236ee4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "def initialize(\n",
    "    PROJECT_ID: str,\n",
    "    setting_file: str,\n",
    "    REGION: str,\n",
    "    bucket_name: str\n",
    "        \n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"PROJECT_ID\", str),  # Return parameters\n",
    "        (\"REGION\", str),\n",
    "        (\"TIMESTAMP\", str),  # Return parameters\n",
    "        (\"API_ENDPOINT\", str),\n",
    "        (\"BQ_PATH\", str),\n",
    "        (\"forecast_file\", str),  # Return parameters\n",
    "        (\"number_prediciton_units\", str),\n",
    "        (\"forecasts_start_date\", str),\n",
    "        (\"actuals_file\", str),\n",
    "        (\"drivers_file\", str),\n",
    "        (\"calendar_file\", str),\n",
    "        (\"files_from_datahub_in\", str),\n",
    "        (\"model_display_name\", str),\n",
    "        (\"dataset_train_name\", str),\n",
    "        (\"training_pipeline_name\", str),\n",
    "        (\"bq_dataset_id_preds\", str),\n",
    "        (\"bq_table_name_preds\", str),\n",
    "        (\"budget_milli_node_hours\", str),\n",
    "        (\"data_granularity_unit\", str),\n",
    "        (\"data_granularity_count\", str),\n",
    "        (\"optimization_objective\", str),\n",
    "        (\"pred_op_path\", str),\n",
    "        (\"past_pred_op_path\", str),\n",
    "        (\"pred_batchjob_disp_name\", str),\n",
    "        (\"past_pred_batchjob_disp_name\", str)\n",
    "    ],\n",
    "):\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    # API Endpoint\n",
    "    #API_ENDPOINT = \"us-central1-aiplatform.googleapis.com\"\n",
    "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "    \n",
    "    BQ_PATH = \"bq://\"+PROJECT_ID+\":\"+bucket_name.replace(\"-\", \"_\")+ \":evaluated_data\"\n",
    "    \n",
    "    \n",
    "    # reading settings.txt file from google cloud storage\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(setting_file)\n",
    "    blob = blob.download_as_string()\n",
    "    blob = blob.decode('utf-8')\n",
    "    lines = blob.splitlines()\n",
    "        \n",
    "    values = {k: v for k, v in (line.split('\\t') for line in lines)}   \n",
    "        \n",
    "    actuals_file = values[\"GCP Day Actual File Name\"].strip()\n",
    "\n",
    "    drivers_file = values[\"GCP Day Driver File\"].strip()\n",
    "    \n",
    "    forecast_file = \"prediction_output.csv\"\n",
    "\n",
    "    calendar_file = values[\"GCP Calendar File\"].strip()\n",
    "\n",
    "    actuals_start_date = values[\"First Day of Actual\"].strip()\n",
    "\n",
    "    forecasts_start_date = values[\"First Day of Forecast\"].strip()\n",
    "\n",
    "    forecasts_last_date = values[\"Last Day of Forecast\"].strip()\n",
    "\n",
    "    number_prediciton_units = values[\"Final: # of Periods to Forecast\"].strip()\n",
    "    \n",
    "    files_from_datahub_in  = values[\"Load Actuals and Drivers from Data Hub\"].strip()\n",
    "    \n",
    "    model_display_name  = values[\"GCP Model Name\"].strip()\n",
    "    \n",
    "    dataset_train_name = \"automl-train-\" + values[\"GCP Dataset Name\"].strip()\n",
    "    \n",
    "    training_pipeline_name = dataset_train_name + \"-pipeline\" \n",
    "    \n",
    "    bq_dataset_id_preds = values[\"GCP Dataset Name\"].strip()\n",
    "    \n",
    "    bq_table_name_preds = bq_dataset_id_preds + \"_predictions\"\n",
    "    \n",
    "    budget_milli_node_hours = str(int(values[\"GCP Computing hours\"].strip())*1000)\n",
    "    \n",
    "    data_granularity_unit = values[\"Time Level\"].strip().lower() #day, minute, hour, week, month, year\n",
    "    \n",
    "    data_granularity_count = \"1\"\n",
    "    \n",
    "    optimization_objective = values[\"GCP Optimization\"].strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #create temporary bucket\n",
    "    bucket_name_temp = bucket_name + \"-temp\" \n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket_temp = storage_client.bucket(bucket_name_temp)\n",
    "    bucket_temp.location = REGION\n",
    "    if bucket_temp.exists():\n",
    "        pass\n",
    "    else: \n",
    "        bucket_temp.create()\n",
    "    \n",
    "    pred_op_path = 'gs://' + bucket_name_temp + \"/\" + TIMESTAMP + \"/\" + \"prediction\"\n",
    "    \n",
    "    past_pred_op_path = 'gs://' + bucket_name_temp + \"/\" +  TIMESTAMP + \"/\" + \"past-prediction\"\n",
    "    \n",
    "    pred_batchjob_disp_name = model_display_name + '-batch'\n",
    "    \n",
    "    past_pred_batchjob_disp_name = model_display_name + '-past-batch'\n",
    "\n",
    "   \n",
    " \n",
    "    return (PROJECT_ID, REGION, TIMESTAMP, API_ENDPOINT, BQ_PATH, forecast_file, number_prediciton_units, forecasts_start_date, actuals_file, drivers_file, calendar_file, files_from_datahub_in, model_display_name, dataset_train_name, training_pipeline_name, bq_dataset_id_preds, bq_table_name_preds, budget_milli_node_hours, data_granularity_unit, data_granularity_count, optimization_objective, pred_op_path, past_pred_op_path, pred_batchjob_disp_name, past_pred_batchjob_disp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4227f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "def transform_data(\n",
    "    bucket_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    TIMESTAMP: str,\n",
    "    actuals_file: str,\n",
    "    drivers_file: str,\n",
    "    calendar_file: str,\n",
    "    forecasts_start_date: str,\n",
    "    number_prediciton_units: str,\n",
    "    files_from_datahub_in: str\n",
    "    \n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"drivers_train_file\", str),  # Return parameters\n",
    "        (\"drivers_prediction_file\", str),\n",
    "        (\"drivers_list_attrib\", str),\n",
    "        (\"drivers_past_prediction_file\", str),\n",
    "        (\"transformations_attrib\", str)\n",
    "    ],\n",
    "):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy  as np\n",
    "    import datetime\n",
    "    import calendar\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    #initializing variables\n",
    "    \n",
    "    \n",
    "    bucket_path = 'gs://' + bucket_name + '/'\n",
    "   \n",
    "    bucket_preprocessed_path = 'gs://' + bucket_name + '-temp' + '/' + TIMESTAMP+ '/' + 'preprocessed-data' + '/'\n",
    "    \n",
    "    \n",
    "    \n",
    "    if files_from_datahub_in == 'true':\n",
    "        files_from_datahub = True\n",
    "    else:\n",
    "        files_from_datahub = False\n",
    "\n",
    "    \n",
    "    # reading the actuals file\n",
    "    df_actuals = pd.read_csv(bucket_path + actuals_file, sep=',', skiprows=[0], header= [0], low_memory=False)\n",
    "    print(\"test\")\n",
    "    #converting the column's format\n",
    "    new_columns = []\n",
    "    for i in df_actuals.columns:\n",
    "        if i == \"Unnamed: 0\":\n",
    "            new_columns.append(\"id\")\n",
    "        else:\n",
    "            new_columns.append(datetime.datetime.strptime(i, '%d %b %y'))\n",
    "    \n",
    "    df_actuals.columns = new_columns\n",
    "    #Find number of days from forecast start date to last day in the actuals file. \n",
    "    # This step is done to get the actual only till one day before forecasting start date. \n",
    "    # E.g. m5 data contains data until May 22, 2019. We are omitting the data starting from forecasting period start date.\n",
    "    # Max date of the list\n",
    "    max_date = max(d for d in new_columns if isinstance(d, datetime.date))\n",
    "    formatted_forecast_start_date = datetime.datetime.strptime(forecasts_start_date, '%Y-%m-%d')\n",
    "    diff_days = max_date - formatted_forecast_start_date\n",
    "\n",
    "    # Find number of date columns in actuals\n",
    "    full_number_columns = len(df_actuals.columns)\n",
    "    number_columns = full_number_columns - diff_days.days - 1\n",
    "    \n",
    "    # transforming the data format and writing it to \"_transformed\" file\n",
    "    df_actuals_transformed = pd.melt(df_actuals, id_vars=df_actuals.columns[0], value_vars=df_actuals.columns[1:number_columns], var_name='date', value_name='label')\n",
    "    df_actuals_transformed.to_csv(bucket_preprocessed_path + actuals_file.split('.')[0] + '_transformed.csv', index=False)\n",
    "\n",
    "\n",
    "    #Work with Calendar file to get the events\n",
    "    calendar_input_file = bucket_path + calendar_file\n",
    "    #calendar_transformed_file = bucket_preprocessed_path + calendar_file.split(' ')[0] + '_'+ calendar_file.split(' ')[1].split('.')[0] + TIMESTAMP + '_transformed.csv'\n",
    "    calendar_transformed_file = bucket_preprocessed_path + calendar_file.split('.')[0] + '_transformed.csv'\n",
    "    # reading the calendar file\n",
    "    df_calendar = pd.read_csv(calendar_input_file, sep=',', skiprows=[-1], header= [0], low_memory=False)\n",
    "   \n",
    "    # transform calendar file\n",
    "    df_calendar_transformed = df_calendar.rename(columns = {'Unnamed: 0': 'date'}, inplace = False)\n",
    "    df_calendar_transformed['date'] = pd.to_datetime(df_calendar_transformed['date'])\n",
    "\n",
    "    #Dropping the event_types if not necessary.\n",
    "    df_calendar_transformed = df_calendar_transformed.drop(columns=['event_type_1', 'event_type_2'])\n",
    "\n",
    "    #Save transformed events per date to csv\n",
    "    df_calendar_transformed.to_csv(calendar_transformed_file, index=False)\n",
    "\n",
    "    drivers_input_file = bucket_path + drivers_file\n",
    "    drivers_train_file = bucket_preprocessed_path + drivers_file.split('.')[0] + '_train.csv'\n",
    "    drivers_prediction_file = bucket_preprocessed_path + drivers_file.split('.')[0] + '_prediction.csv'\n",
    "\n",
    "    # reading the drivers file\n",
    "    # skipping first 2 rows\n",
    "    if files_from_datahub:\n",
    "        df_drivers = pd.read_csv( drivers_input_file, sep=',', skiprows=[-1], header= [0], low_memory=False)\n",
    "    else:\n",
    "        df_drivers = pd.read_csv( drivers_input_file, sep=',', skiprows=[0], header= [0], low_memory=False)\n",
    "\n",
    "\n",
    "    #converting the column's format\n",
    "    new_columns = []\n",
    "    for i in df_drivers.columns:\n",
    "        if i == \"Unnamed: 0\":\n",
    "            new_columns.append(\"drivers\")\n",
    "        elif i == \"Unnamed: 1\":\n",
    "            new_columns.append(\"id\")\n",
    "        else:\n",
    "            new_columns.append(datetime.datetime.strptime(i, '%d %b %y'))\n",
    "\n",
    "    df_drivers.columns = new_columns\n",
    "\n",
    "    # splitting the drivers data to train and prediction data\n",
    "    last_column_index_train = number_columns + 1\n",
    "\n",
    "    #Find the indices of labelled and unlabelled prediction data to send to prediction batch job\n",
    "    first_column_index_test = last_column_index_train - int(number_prediciton_units)\n",
    "    last_column_index_test = last_column_index_train + int(number_prediciton_units)\n",
    "\n",
    "\n",
    "    calendar_columns = []\n",
    "    drivers_train = []\n",
    "    drivers_prediction = []\n",
    "    drivers_list = []\n",
    "    transformations = []\n",
    "\n",
    "    for i in df_calendar_transformed.columns:\n",
    "        if i != \"date\":\n",
    "            calendar_columns.append(i.lower().replace(' ', '_'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for driver, df_driver in df_drivers.groupby('drivers'):\n",
    "        drivers_list.append(driver.lower().replace(' ', '_'))\n",
    "        drivers_train.append(pd.melt(df_driver, id_vars=df_driver.columns[1], value_vars=df_driver.columns[2:last_column_index_train], var_name='date', value_name=driver))\n",
    "        drivers_prediction.append(pd.melt(df_driver, id_vars=df_driver.columns[1], value_vars=df_driver.columns[first_column_index_test:last_column_index_test], var_name='date', value_name=driver))\n",
    "\n",
    "    #Add more events/features/drivers from calendar dataset to the events in the drivers dataset.\n",
    "    drivers_list.extend(calendar_columns)\n",
    "\n",
    "    #Transforming and merging the drivers data together\n",
    "\n",
    "    drivers_train_merged_1 = drivers_train[0]\n",
    "\n",
    "    for df_ in drivers_train[1:]:  \n",
    "        drivers_train_merged_1 = drivers_train_merged_1.merge(df_, how=\"outer\", on =['id', 'date'])\n",
    "\n",
    "    # add more drivers to training file\n",
    "    drivers_train_merged = drivers_train_merged_1.merge(df_calendar_transformed, how=\"left\", on =['date'])\n",
    "\n",
    "    # transform prediction file\n",
    "    drivers_prediction_merged_1 = drivers_prediction[0]\n",
    "\n",
    "\n",
    "    for df_ in drivers_prediction[1:]:  \n",
    "        drivers_prediction_merged_1 = drivers_prediction_merged_1.merge(df_, how=\"outer\", on =['id', 'date'])\n",
    "\n",
    "    # add calendar events to prediction drivers\n",
    "    drivers_prediction_merged = drivers_prediction_merged_1.merge(df_calendar_transformed, how=\"left\", on =['date'])\n",
    "\n",
    "\n",
    "    # writing the drivers for the prediction part directly to a file \n",
    "    #NaN = np.nan\n",
    "    #drivers_prediction_merged[\"label\"] = NaN\n",
    "    drivers_predictions_merged_labels = drivers_prediction_merged.merge(df_actuals_transformed, how=\"left\", on =['id', 'date'])\n",
    "    drivers_predictions_merged_labels.columns = [x.lower().replace(' ', '_') for x in drivers_predictions_merged_labels.columns]\n",
    "    drivers_predictions_merged_labels.to_csv(drivers_prediction_file, index=False)\n",
    "\n",
    "\n",
    "    # merging the drivers train part with Actuals data and write it to GCS\n",
    "    drivers_train_final = df_actuals_transformed.merge(drivers_train_merged, how=\"outer\", on=[\"id\", \"date\"])\n",
    "    drivers_train_final.columns = [x.lower().replace(' ', '_') for x in drivers_train_final.columns]\n",
    "    drivers_train_final.to_csv(drivers_train_file, index=False)\n",
    "\n",
    "    ## Prepare past prediction file\n",
    "    drivers_past_prediction_file = bucket_preprocessed_path + drivers_file.split('.')[0] + '_past_prediction.csv'\n",
    "\n",
    "    # reading the actuals file\n",
    "    df_actuals = pd.read_csv(bucket_path + actuals_file, sep=',', skiprows=[0], header= [0], low_memory=False)\n",
    "\n",
    "    #converting the column's format\n",
    "    new_columns = []\n",
    "    for i in df_actuals.columns:\n",
    "        if i == \"Unnamed: 0\":\n",
    "            new_columns.append(\"id\")\n",
    "        else:\n",
    "            new_columns.append(datetime.datetime.strptime(i, '%d %b %y'))\n",
    "\n",
    "    df_actuals.columns = new_columns\n",
    "\n",
    "    max_date_past = max(d for d in new_columns if isinstance(d, datetime.date))\n",
    "    formatted_past_forecast_start_date = datetime.datetime.strptime(forecasts_start_date, '%Y-%m-%d')\n",
    "    diff_days_past = max_date_past - formatted_past_forecast_start_date\n",
    "\n",
    "\n",
    "    # Find number of date columns in actuals\n",
    "    full_number_columns_past = len(df_actuals.columns)\n",
    "    number_columns_past = full_number_columns_past - diff_days_past.days - int(number_prediciton_units)\n",
    "\n",
    "\n",
    "    # transforming the data format and writing it to \"_transformed\" file\n",
    "    df_actuals_transformed_past = pd.melt(df_actuals, id_vars=df_actuals.columns[0], value_vars=df_actuals.columns[1:number_columns_past], var_name='date', value_name='label')\n",
    "\n",
    "\n",
    "    # splitting the drivers data to train and prediction data\n",
    "    last_column_index_train = number_columns_past \n",
    "    first_column_index_test = last_column_index_train - int(number_prediciton_units)\n",
    "    last_column_index_test = last_column_index_train + int(number_prediciton_units)\n",
    "\n",
    "    # Now prepare the predictions file for the past period.\n",
    "    # Create past data prediction file indices for labelled and unlabelled data.\n",
    "    past_first_column_index_test = first_column_index_test - int(number_prediciton_units)\n",
    "    past_last_column_index_test = last_column_index_test + int(number_prediciton_units)\n",
    "\n",
    "\n",
    "\n",
    "    #  Working with the past predictions file for the past period.\n",
    "\n",
    "    drivers_prediction_past = []\n",
    "\n",
    "    for driver, df_driver in df_drivers.groupby('drivers'):\n",
    "        #drivers_prediction_past.append(pd.melt(df_driver, id_vars=df_driver.columns[1], value_vars=df_driver.columns[first_column_index_test:past_last_column_index_test], var_name='date', value_name=driver))\n",
    "        drivers_prediction_past.append(pd.melt(df_driver, id_vars=df_driver.columns[1], value_vars=df_driver.columns[first_column_index_test:last_column_index_test], var_name='date', value_name=driver))\n",
    "\n",
    "    # transform prediction file\n",
    "    drivers_prediction_past_merged = drivers_prediction_past[0]\n",
    "\n",
    "    for df_ in drivers_prediction_past[1:]:  \n",
    "        drivers_prediction_past_merged = drivers_prediction_past_merged.merge(df_, how=\"outer\", on =['id', 'date'])\n",
    "\n",
    "    # add calendar events to prediction drivers\n",
    "    drivers_prediction_past_merged = drivers_prediction_past_merged.merge(df_calendar_transformed, how=\"left\", on =['date'])\n",
    "\n",
    "    # writing the drivers for the prediction part directly to a file \n",
    "    #NaN = np.nan\n",
    "    #drivers_prediction_merged[\"label\"] = NaN\n",
    "    drivers_prediction_past_merged_labels = drivers_prediction_past_merged.merge(df_actuals_transformed_past, how=\"left\", on =['id', 'date'])\n",
    "    drivers_prediction_past_merged_labels.columns = [x.lower().replace(' ', '_') for x in drivers_prediction_past_merged_labels.columns]\n",
    "    drivers_prediction_past_merged_labels.to_csv(drivers_past_prediction_file, index=False)\n",
    "    \n",
    "    #import copy\n",
    "    import json\n",
    "    #drivers = copy.copy(drivers_list)\n",
    "    transformations = [\n",
    "        #{\"auto\": {\"column_name\": \"id\"}},\n",
    "        {\"auto\": {\"column_name\": \"date\"}},\n",
    "        {\"auto\": {\"column_name\": \"label\"}}\n",
    "        ]\n",
    "\n",
    "    for driver in drivers_list:\n",
    "        driver_json = '{\"auto\": { \"column_name\": \"'+ driver.lower().replace(' ', '_') + '\"}}'\n",
    "        transformations.append(json.loads(driver_json))\n",
    "    \n",
    "    transformations_attrib = json.dumps(transformations)\n",
    "    #transformations_1 = str(transformations).replace(\"[]\",\"\")\n",
    "    \n",
    "    driverslist = drivers_list\n",
    "    driverslist.append(\"date\")\n",
    "    \n",
    "    drivers_list_attrib = json.dumps(driverslist)\n",
    "    \n",
    "    return (drivers_train_file, drivers_prediction_file, drivers_list_attrib,  drivers_past_prediction_file, transformations_attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14e23fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    packages_to_install = [\n",
    "        \"google-cloud-aiplatform\"\n",
    "    ],\n",
    ")\n",
    "def model_evaluation(\n",
    "    model: Input[Model],\n",
    "    API_ENDPOINT: str\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import gapic as aip\n",
    "    from google.protobuf import json_format\n",
    "    import json\n",
    "    import logging\n",
    "    \n",
    "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "    print(\"model name parameter: \", model)\n",
    "    def create_model_client():\n",
    "        client = aip.ModelServiceClient(\n",
    "            client_options=client_options\n",
    "        )\n",
    "        return client\n",
    "    client = create_model_client()\n",
    "    \n",
    "    def list_model_evaluations(name):\n",
    "        response = client.list_model_evaluations(parent=name)\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = json_format.MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                print(metric)\n",
    "            print('rootMeanSquaredError', metrics['rootMeanSquaredError'])\n",
    "            print('meanAbsoluteError', metrics['meanAbsoluteError'])\n",
    "\n",
    "        return evaluation.name\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    #aiplatform.init(project=project)\n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    #last_evaluation = list_model_evaluations(model_to_deploy_name)\n",
    "    last_evaluation = list_model_evaluations(model_resource_path)\n",
    "    def model_evaluation(name):\n",
    "        response = client.get_model_evaluation(name=name)\n",
    "        print(\"response\")\n",
    "        print(\" name:\", response.name)\n",
    "        print(\" metrics_schema_uri:\", response.metrics_schema_uri)\n",
    "        print(\" metrics:\", json_format.MessageToDict(response._pb.metrics))\n",
    "        print(\" create_time:\", response.create_time)\n",
    "\n",
    "\n",
    "    model_evaluation(last_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f03cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "def append_results(\n",
    "    MODEL_NAME: str,\n",
    "    forecast_file: str,\n",
    "    BUCKET_NAME: str,\n",
    "    BUCKET_NAME_PAST: str,\n",
    "    TIMESTAMP: str,\n",
    "    current_batch_prediction_job: Input[Artifact],\n",
    "    past_batch_prediction_job: Input[Artifact]\n",
    "    \n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "\n",
    "    prediction_path = BUCKET_NAME + \"/prediction-\"+MODEL_NAME.replace(\"-\", \"_\")+\"*/predictions*.csv\"\n",
    "\n",
    "    print(prediction_path)\n",
    "    predictions = pd.read_csv(prediction_path, usecols= ['date','id','predicted_label'])\n",
    "    predictions.columns = ['date','id',MODEL_NAME]\n",
    "\n",
    "    past_prediction_path = BUCKET_NAME_PAST + \"/prediction-\"+MODEL_NAME.replace(\"-\", \"_\")+\"*/predictions*.csv\"\n",
    "    past_predictions = pd.read_csv(past_prediction_path, usecols= ['date','id','predicted_label'])\n",
    "    past_predictions.columns = ['date','id',MODEL_NAME]\n",
    "\n",
    "\n",
    "    final_result = predictions.append(past_predictions)\n",
    "    final_result.sort_values(by=['date']).to_csv(BUCKET_NAME + \"/\" + forecast_file, index=False)\n",
    "\n",
    "    # Final result file path:\n",
    "    final_result_path = BUCKET_NAME + \"/\" + forecast_file\n",
    "    return final_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26df29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "def load_final_result_to_big_query(\n",
    "    forecasts_start_date: str,\n",
    "    REGION: str,\n",
    "    final_result_path: str,\n",
    "    project_id: str,\n",
    "    bq_dataset_id_preds: str,\n",
    "    bq_table_name_preds: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    \n",
    "\n",
    "\n",
    "    # Make clients.\n",
    "    bq_client = bigquery.Client(project=project_id, location=REGION)\n",
    "   \n",
    "    print(\"Client creating using default project: {}\".format(bq_client.project))\n",
    "    \n",
    "  \n",
    "    # Define a name for the new dataset.\n",
    "    dataset_id = bq_dataset_id_preds # BigQuery dataset name. Replace with your BQ dataset name.\n",
    "\n",
    "\n",
    "    try:\n",
    "        dataset = bq_client.get_dataset(dataset_id)  # Make an API request.\n",
    "        print(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except NotFound:\n",
    "        print(\"Dataset {} is not found\".format(dataset_id))\n",
    "        #The project defaults to the Client's project if not specified.\n",
    "        dataset = bq_client.create_dataset(dataset_id)  # API request\n",
    "        print(\"Created Dataset {}\".format(dataset_id))\n",
    "\n",
    "      \n",
    "    ## Set up variables for BigQuery load: \n",
    "    FINAL_TABLE_NAME = bq_table_name_preds\n",
    "    \n",
    "    # Configure the load job to load from Google Cloud Storage bucket. Here, we create the schema of table and load the data from a GCS bucket.\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField('date', 'DATE'),\n",
    "            bigquery.SchemaField('id', 'STRING'),\n",
    "            bigquery.SchemaField('predictions', 'FLOAT')\n",
    "        ],\n",
    "        skip_leading_rows=1,\n",
    "        # The source format defaults to CSV. The line below is optional.\n",
    "        source_format=bigquery.SourceFormat.CSV\n",
    "    )\n",
    "    uri = final_result_path\n",
    "    destination_table_ref = dataset.table(FINAL_TABLE_NAME)\n",
    "    table_ref = dataset.table(FINAL_TABLE_NAME)\n",
    "    \n",
    "    table_id=\"{}.{}.{}\".format(project_id,dataset_id,FINAL_TABLE_NAME)\n",
    "    try:\n",
    "        bq_client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Table {} already exists.\".format(table_id))\n",
    "        bq_client.delete_table(table_id)\n",
    "    except NotFound:\n",
    "        print(\"Table {} is not found.\".format(table_id))\n",
    "\n",
    "    # Start the load job\n",
    "    load_job = bq_client.load_table_from_uri(\n",
    "        uri, destination_table_ref, job_config=job_config)\n",
    "    print('Starting job {}'.format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print('Job finished.')\n",
    "\n",
    "    # Retreive the destination table\n",
    "    destination_table = bq_client.get_table(table_ref)\n",
    "    print('Loaded {} rows.'.format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fecfeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-test-1\"\n",
    ")\n",
    "def pipeline( \n",
    "    project: str, \n",
    "    bucket_name: str,\n",
    "    region: str,\n",
    "    setting_file: str\n",
    "):\n",
    "    variables = initialize(project, setting_file, region, bucket_name)\n",
    "    transformed_data = transform_data(\n",
    "        bucket_name,  \n",
    "        project, \n",
    "        region, \n",
    "        variables.outputs[\"TIMESTAMP\"],\n",
    "        variables.outputs[\"actuals_file\"],\n",
    "        variables.outputs[\"drivers_file\"],\n",
    "        variables.outputs[\"calendar_file\"],\n",
    "        variables.outputs[\"forecasts_start_date\"],\n",
    "        variables.outputs[\"number_prediciton_units\"],\n",
    "        variables.outputs[\"files_from_datahub_in\"]).set_memory_limit('64G')\n",
    "    dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "        project=project, \n",
    "        display_name=variables.outputs[\"dataset_train_name\"], \n",
    "        gcs_source=transformed_data.outputs[\"drivers_train_file\"], \n",
    "        location=region)\n",
    "    training_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=variables.outputs[\"training_pipeline_name\"],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        column_transformations=transformed_data.outputs[\"transformations_attrib\"],\n",
    "        optimization_objective=variables.outputs[\"optimization_objective\"],\n",
    "        budget_milli_node_hours=variables.outputs[\"budget_milli_node_hours\"], #needs to be modified using new parameters\n",
    "        target_column=\"label\", \n",
    "        time_column=\"date\",\n",
    "        time_series_identifier_column=\"id\",\n",
    "        unavailable_at_forecast_columns=[\"label\"],\n",
    "        available_at_forecast_columns=transformed_data.outputs[\"drivers_list_attrib\"],\n",
    "        forecast_horizon=variables.outputs[\"number_prediciton_units\"],\n",
    "        data_granularity_unit=variables.outputs[\"data_granularity_unit\"],\n",
    "        data_granularity_count=variables.outputs[\"data_granularity_count\"],\n",
    "        #predefined_split_column_name=\n",
    "        #weight_column=\n",
    "        time_series_attribute_columns=None,\n",
    "        context_window=variables.outputs[\"number_prediciton_units\"],\n",
    "        export_evaluated_data_items=True, #False\n",
    "        export_evaluated_data_items_bigquery_destination_uri=variables.outputs[\"BQ_PATH\"],\n",
    "        #\"bq://\"+project+\":\"+bucket_name.replace(\"-\", \"_\")+ \":evaluated_data\",\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        #quantiles=\n",
    "        #validation_options=\n",
    "        model_display_name=variables.outputs[\"model_display_name\"])\n",
    "    model_evaluation_op = model_evaluation(training_op.outputs[\"model\"], variables.outputs[\"API_ENDPOINT\"])\n",
    "    current_batch_predict_op = gcc_aip.ModelBatchPredictOp(\n",
    "        model=training_op.outputs[\"model\"], \n",
    "        gcs_source=transformed_data.outputs[\"drivers_prediction_file\"], \n",
    "        gcs_destination_prefix=variables.outputs[\"pred_op_path\"],\n",
    "        instances_format= 'csv',\n",
    "        predictions_format= 'csv',\n",
    "        job_display_name=variables.outputs[\"pred_batchjob_disp_name\"])\n",
    "    past_batch_predict_op = gcc_aip.ModelBatchPredictOp(\n",
    "        model=training_op.outputs[\"model\"], \n",
    "        gcs_source=transformed_data.outputs[\"drivers_past_prediction_file\"], \n",
    "        gcs_destination_prefix=variables.outputs[\"past_pred_op_path\"],\n",
    "        instances_format= 'csv',\n",
    "        predictions_format= 'csv',\n",
    "        job_display_name=variables.outputs[\"past_pred_batchjob_disp_name\"])\n",
    "    append_results_op = append_results(\n",
    "        variables.outputs[\"model_display_name\"],\n",
    "        variables.outputs[\"forecast_file\"],\n",
    "        variables.outputs[\"pred_op_path\"],\n",
    "        variables.outputs[\"past_pred_op_path\"],\n",
    "        variables.outputs[\"TIMESTAMP\"],\n",
    "        current_batch_predict_op.outputs[\"batchpredictionjob\"],\n",
    "        past_batch_predict_op.outputs[\"batchpredictionjob\"]\n",
    "    )\n",
    "    load_final_result_to_big_query(\n",
    "        variables.outputs[\"forecasts_start_date\"],\n",
    "        variables.outputs[\"REGION\"],\n",
    "        append_results_op.output,\n",
    "        variables.outputs[\"PROJECT_ID\"],\n",
    "        variables.outputs[\"bq_dataset_id_preds\"],\n",
    "        variables.outputs[\"bq_table_name_preds\"]\n",
    "    )\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='test_pipe.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0199be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-test-1-20210907124011?project=932447320071\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/932447320071/locations/us-central1/pipelineJobs/pipeline-test-1-20210907124011\n"
     ]
    }
   ],
   "source": [
    "api_client = ap.PipelineJob(\n",
    "            display_name=\"pipeline-job-test\",\n",
    "            template_path=\"test_pipe.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            project=PROJECT_ID,\n",
    "            location=REGION,\n",
    "            enable_caching= False,\n",
    "            parameter_values={\"project\": PROJECT_ID, \"bucket_name\": DATA_BUCKET, \"region\": REGION, \"setting_file\": SETTING_FILE}\n",
    "            \n",
    "        )\n",
    "\n",
    "# Run the pipeline specifying the Service Account\n",
    "api_client.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59e855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
