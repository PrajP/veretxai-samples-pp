{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8d5949-1e8c-493a-b89d-5f960daf5f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4925e5-c4d9-4a84-bae7-9dcb24a6fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f7d060-0889-4758-896b-f1a141b5b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171d3e79-e8ec-4d6e-ac89-8bdca26a4ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Todays date:  20220822170753\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world!\")\n",
    "print(\"Todays date: \", TIMESTAMP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546c2792-8871-4fdb-9a6a-421934e180b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"\"\n",
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713bb8c8-d87f-4a86-a9db-114a2ed551cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING\"):\n",
    "    \"\"\"\n",
    "    The testing suite does not currently support testing on Dataproc clusters,\n",
    "    so the testing environment is setup to replicate Dataproc via the following steps.\n",
    "    \"\"\"\n",
    "    JAVA_VER = \"8u332-b09\"\n",
    "    JAVA_FOLDER = \"/tmp/java\"\n",
    "    FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
    "    TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
    "    DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
    "    PYSPARK_VER = \"3.1.3\"\n",
    "\n",
    "    # Download Open JDK 8. Spark requires Java to execute.\n",
    "    ! rm -rf $JAVA_FOLDER\n",
    "    ! mkdir $JAVA_FOLDER\n",
    "    ! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
    "    os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
    "    ! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
    "    ! echo $JAVA_HOME\n",
    "\n",
    "    # Pin the Spark version to match that the Dataproc 2.0 cluster.\n",
    "    ! pip install {USER_FLAG} pyspark==$PYSPARK_VER -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb6a96d-ddd8-4d00-974a-d963067a98a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.1.3\n",
      "  Using cached pyspark-3.1.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/jupyter/.local/lib/python3.7/site-packages (from pyspark==3.1.3) (0.10.9)\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install {USER_FLAG} pyspark==\"3.1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43659537-086a-4f4f-8ca8-6b22f0d4ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced06054-3278-4416-9390-c2ae1fedb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71c7948-b6a0-40f4-9b64-87416326de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ListKernelSpecs] WARNING | Config option `kernel_spec_manager_class` not recognized by `ListKernelSpecs`.\n",
      "{\n",
      "  \"kernelspecs\": {\n",
      "    \"python3\": {\n",
      "      \"resource_dir\": \"/opt/conda/share/jupyter/kernels/python3\",\n",
      "      \"spec\": {\n",
      "        \"argv\": [\n",
      "          \"/bin/bash\",\n",
      "          \"-c\",\n",
      "          \"PYSPARK_DRIVER_PYTHON_OPTS='kernel -f {connection_file}' pyspark\"\n",
      "        ],\n",
      "        \"env\": {\n",
      "          \"PYSPARK_DRIVER_PYTHON\": \"/opt/conda/bin/ipython\",\n",
      "          \"PYSPARK_PYTHON\": \"/opt/conda/bin/python\"\n",
      "        },\n",
      "        \"display_name\": \"PySpark\",\n",
      "        \"language\": \"python\",\n",
      "        \"interrupt_mode\": \"signal\",\n",
      "        \"metadata\": {}\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list --json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8997b34e-5873-4f64-9f23-f3fc27fea61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER_NAME: my-dataproc-cluster-01\n",
      "CLUSTER_REGION: us-central1\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    CLUSTER_NAME = \"my-dataproc-cluster-01\"  # @param {type: \"string\"}\n",
    "    CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "    if CLUSTER_REGION == \"[your-region]\":\n",
    "        CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "    print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "    print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d7df8b-a247-43d0-be2a-7f3f4ead452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-and-spark-demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    #shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    #PROJECT_ID = shell_output[0]\n",
    "    PROJECT_ID = \"vertex-and-spark-demo\"\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa816e5e-9776-400e-a07b-6724406cc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"vertex-and-spark-demo\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715e91cc-367f-43cc-9558-80971369640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f169ca73-8203-4aab-beb7-31de191d5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    DATASET_NAME = f\"spark_dataset_{TIMESTAMP}\"  # @param {type:\"string\"}\n",
    "\n",
    "    if (\n",
    "        DATASET_NAME == \"\"\n",
    "        or DATASET_NAME is None\n",
    "        or DATASET_NAME == \"[your-dataset-name]\"\n",
    "    ):\n",
    "        DATASET_NAME = f\"{PROJECT_ID}{UUID}\"\n",
    "else:\n",
    "    DATASET_NAME = f\"python_docs_samples_tests_spark_{UUID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5781cd1a-1b54-47d0-b39a-fc74c36fd192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_dataset_20220822170753'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f72aa8-ab86-43dd-a3b0-3007b34aca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d09cb1d-7444-4a03-80e7-3c4b825b2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3513dbcb-98f5-418e-91b8-22f3de13a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable bigquery.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45293bdc-ed30-4c33-8dd6-cbf4fbdd7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: BigQuery API has not been used in project\n",
      "457198359346 before or it is disabled. Enable it by visiting https://console.dev\n",
      "elopers.google.com/apis/api/bigquery.googleapis.com/overview?project=45719835934\n",
      "6 then retry. If you enabled this API recently, wait a few minutes for the\n",
      "action to propagate to our systems and retry.\n"
     ]
    }
   ],
   "source": [
    "!bq mk --location=$REGION $PROJECT_ID:$DATASET_NAME \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a6d59-bc6f-4a31-8acd-8f8ea92ec4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You use Spark SQL in a \"SparkSession\" to create DataFrames\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import avg, col, count, desc, round, size, udf\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4031bba4-de4d-47b4-94cf-42623a502227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"SparkSession\" with the following config.\n",
    "VER = \"0.26.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "895d11d3-04e6-4806-b74b-2d8cc977d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName('spark-bigquery-demo') \\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80762a9a-22eb-4e63-804d-221b1e606689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the connector.\n",
    "bucket = \"vertex-and-spark-demo-bucket\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f612cf-ea46-44bc-8c89-b5c8766371e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from BigQuery.\n",
    "words = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n",
    "  .load()\n",
    "words.createOrReplaceTempView('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ecd401-c02e-4d5b-9785-c642e033b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/22 17:19:57 WARN org.apache.hadoop.hive.metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "[Stage 0:===========================================================(1 + 0) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     word|word_count|\n",
      "+---------+----------+\n",
      "|     XVII|         2|\n",
      "|    spoil|        28|\n",
      "|    Drink|         7|\n",
      "|forgetful|         5|\n",
      "|   Cannot|        46|\n",
      "|    cures|        10|\n",
      "|   harder|        13|\n",
      "|  tresses|         3|\n",
      "|      few|        62|\n",
      "|  steel'd|         5|\n",
      "| tripping|         7|\n",
      "|   travel|        35|\n",
      "|   ransom|        55|\n",
      "|     hope|       366|\n",
      "|       By|       816|\n",
      "|     some|      1169|\n",
      "|    those|       508|\n",
      "|    still|       567|\n",
      "|      art|       893|\n",
      "|    feign|        10|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- word: string (nullable = false)\n",
      " |-- word_count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform word count.\n",
    "word_count = spark.sql(\n",
    "    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\n",
    "word_count.show()\n",
    "word_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c4995c-ad31-4d36-b83c-f18445ef8de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the data to BigQuery\n",
    "word_count.write.format('bigquery') \\\n",
    "  .option('table', 'wordcount_dataset.wordcount_output') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1c2ee-db98-4e87-8723-38648dcbb0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"SparkSession\" with the following config.\n",
    "VER = \"0.26.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    connector = f\"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/{VER}/{FILE_NAME}\"\n",
    "else:\n",
    "    connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f11935-d83c-4ad9-b52a-a7d30b73413a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
